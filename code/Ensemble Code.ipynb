{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"0408_Code_v4.EnsembleCode.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"conscious-democrat"},"source":["## Set Environment"],"id":"conscious-democrat"},{"cell_type":"code","metadata":{"scrolled":true,"tags":[],"id":"objective-sympathy","outputId":"c0e774a2-17c5-4e7b-887d-86264b2091ff"},"source":["# Dependency Env Setting\n","!pip install efficientnet_pytorch"],"id":"objective-sympathy","execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: efficientnet_pytorch in /opt/conda/lib/python3.7/site-packages (0.7.0)\n","Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet_pytorch) (1.6.0)\n","Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (0.18.2)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (1.18.5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wooden-sport","outputId":"12b23a2c-1909-426d-ee14-4c00c80397bf"},"source":["# Import libraries\n","import pandas as pd\n","import numpy as np\n","import cv2\n","import PIL\n","import os\n","import time\n","import glob\n","import pickle\n","import random\n","from pathlib import Path\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","from torch.utils.data import DataLoader, Dataset, Subset\n","from efficientnet_pytorch import EfficientNet\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import f1_score\n","\n","# Set random seed\n","seed = 42\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = True\n","print(f'seed : {seed}')\n","\n","# Set device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f'device : {device}')\n","print(torch.cuda.get_device_properties(device))\n","\n","# Set ROOT_PATH\n","ROOT_PATH = os.getcwd()\n","print(f'ROOT_PATH : {ROOT_PATH}')"],"id":"wooden-sport","execution_count":null,"outputs":[{"output_type":"stream","text":["seed : 42\n","device : cuda:0\n","_CudaDeviceProperties(name='Tesla P40', major=6, minor=1, total_memory=24451MB, multi_processor_count=30)\n","ROOT_PATH : /opt/ml\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"provincial-liberal"},"source":["## Define Dataset"],"id":"provincial-liberal"},{"cell_type":"code","metadata":{"id":"aquatic-tobago"},"source":["# Define Dataset\n","class FaceDataset(Dataset) :\n","    def __init__(self, img_paths, trsf, aug=None, training=False) :\n","        self.img_paths = img_paths\n","        self.trsf = trsf\n","        self.aug = aug\n","        self.training = training\n","    \n","    def __len__(self) :\n","        return len(self.img_paths)\n","\n","    def __getitem__(self, idx) :\n","        img = cv2.imread(self.img_paths[idx])\n","        \n","        if self.aug is not None :\n","            img = self.aug(image=img)['image']\n","            img = self.trsf(img)\n","        else : \n","            img = self.trsf(img)\n","        \n","        if self.training :\n","            label = labeling(self.img_paths[idx])\n","            return {'image' : img, 'label' : label}\n","        else :\n","            return {'image' : img}"],"id":"aquatic-tobago","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"unauthorized-banana"},"source":["## Define Model "],"id":"unauthorized-banana"},{"cell_type":"code","metadata":{"id":"solid-organ"},"source":["# Deffine Model\n","class MyEfficientNet(nn.Module) :\n","    def __init__(self) :\n","        super(MyEfficientNet, self).__init__()\n","        self.EFF = EfficientNet.from_pretrained('efficientnet-b4', in_channels=3, num_classes=18)\n","    \n","    def forward(self, x) :\n","        x = self.EFF(x)\n","        x = F.softmax(x, dim=1)\n","        return x"],"id":"solid-organ","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"awful-mystery"},"source":["## Define Transform"],"id":"awful-mystery"},{"cell_type":"code","metadata":{"id":"racial-tissue"},"source":["class ToTensor(object) :\n","    def __call__(self, img) :\n","        img = np.array(img)/255\n","        img = img.transpose((2,0,1))\n","        img = torch.FloatTensor(img)\n","        return img"],"id":"racial-tissue","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"approved-shower"},"source":["# Set Transform\n","'각 Model의 Crop Size에 해당하는 Transform을 정의합니다.'\n","small_trsf = T.Compose([\n","    T.ToPILImage(),\n","    T.CenterCrop([300,256]),\n","    ToTensor(),\n","    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n","])\n","\n","midium_trsf = T.Compose([\n","    T.ToPILImage(),\n","    T.CenterCrop([384,320]),\n","    ToTensor(),\n","    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n","])\n","\n","big_trsf = T.Compose([\n","    ToTensor(),\n","    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n","])"],"id":"approved-shower","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hungarian-pharmaceutical"},"source":["## Inference"],"id":"hungarian-pharmaceutical"},{"cell_type":"code","metadata":{"id":"tutorial-invalid","outputId":"ff0f9561-227a-4595-fe02-f3f364563895"},"source":["# Ensemble Inference\n","os.chdir(ROOT_PATH)\n","submission = pd.read_csv('input/data/eval/info.csv')\n","test_img_paths = [os.path.join('input/data/eval/images', img_file) for img_file in submission.ImageID]\n","prediction_lst = []\n","\n","'최종으로 사용할 Model의 Directory입니다.'\n","'28. Dropout, B4, NCC'\n","'27. Dropout, B4, CC'\n","'26. Dropout, B4'\n","'25. Seed43, B4, Baseline'\n","\n","best_dirs=['28. Dropout, B4, NCC', '27. Dropout, B4, CC', '26. Dropout, B4', '25. Seed43, B4, Baseline']\n","for best_dir in best_dirs :\n","    if 'NCC' in best_dir  :\n","        print(best_dir, ' : big')\n","        dataset = FaceDataset(test_img_paths, big_trsf, training=False)\n","    elif 'CC' in best_dir :\n","        print(best_dir, ' : midium')\n","        dataset = FaceDataset(test_img_paths, midium_trsf, training=False)\n","    else :\n","        print(best_dir, ' : small')\n","        dataset = FaceDataset(test_img_paths, small_trsf, training=False)\n","    loader = DataLoader(dataset=dataset, batch_size=16, shuffle=False)\n","    \n","    best_models = glob.glob(f'custom_data/{best_dir}/*.pth')\n","    for best_model in best_models :\n","        model = MyEfficientNet()\n","        model.load_state_dict(torch.load(best_model))\n","        model.to(device)\n","        model.eval()\n","        prediction_array=[]\n","\n","        with tqdm(loader,\n","                 total=loader.__len__(),\n","                 unit='batch') as test_bar :\n","            for sample in test_bar :\n","                imgs = sample['image'].float().to(device)\n","                probs = model(imgs)\n","                probs = probs.cpu().detach().numpy()\n","                prediction_array.extend(probs)\n","\n","        prediction_lst.append(np.array(prediction_array)[...,np.newaxis])\n","\n","submission['ans'] = np.argmax(np.mean(np.concatenate(prediction_lst, axis=2), axis=2), axis=1)\n","submission.to_csv(f'custom_data/{name}/{name}.csv', index=False)"],"id":"tutorial-invalid","execution_count":null,"outputs":[{"output_type":"stream","text":["28. Dropout, B4, NCC  : big\n","Loaded pretrained weights for efficientnet-b4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 788/788 [04:51<00:00,  2.71batch/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded pretrained weights for efficientnet-b4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 788/788 [04:49<00:00,  2.72batch/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded pretrained weights for efficientnet-b4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 788/788 [04:50<00:00,  2.71batch/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded pretrained weights for efficientnet-b4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 788/788 [04:50<00:00,  2.71batch/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded pretrained weights for efficientnet-b4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 788/788 [04:52<00:00,  2.70batch/s]\n"],"name":"stderr"},{"output_type":"stream","text":["27. Dropout, B4, CC  : midium\n","Loaded pretrained weights for efficientnet-b4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 788/788 [03:20<00:00,  3.93batch/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded pretrained weights for efficientnet-b4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 788/788 [03:20<00:00,  3.94batch/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded pretrained weights for efficientnet-b4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 788/788 [03:19<00:00,  3.94batch/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded pretrained weights for efficientnet-b4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 788/788 [03:19<00:00,  3.95batch/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded pretrained weights for efficientnet-b4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 788/788 [03:19<00:00,  3.95batch/s]\n"],"name":"stderr"},{"output_type":"stream","text":["26. Dropout, B4  : small\n","Loaded pretrained weights for efficientnet-b4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 788/788 [02:23<00:00,  5.50batch/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded pretrained weights for efficientnet-b4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 788/788 [02:22<00:00,  5.52batch/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded pretrained weights for efficientnet-b4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 788/788 [02:23<00:00,  5.48batch/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded pretrained weights for efficientnet-b4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 788/788 [02:22<00:00,  5.52batch/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded pretrained weights for efficientnet-b4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 788/788 [02:22<00:00,  5.52batch/s]\n"],"name":"stderr"},{"output_type":"stream","text":["25. Seed43, B4, Baseline  : small\n","Loaded pretrained weights for efficientnet-b4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 788/788 [02:23<00:00,  5.50batch/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded pretrained weights for efficientnet-b4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 788/788 [02:22<00:00,  5.51batch/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded pretrained weights for efficientnet-b4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 788/788 [02:22<00:00,  5.52batch/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded pretrained weights for efficientnet-b4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 788/788 [02:24<00:00,  5.47batch/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded pretrained weights for efficientnet-b4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 788/788 [02:22<00:00,  5.52batch/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"imposed-extreme"},"source":["Dropout_big_proba = prediction_lst[0:5]\n","Dropout_midium_proba = prediction_lst[5:10]\n","Dropout_small_proba = prediction_lst[10:15]\n","small_proba = prediction_lst[15:20]"],"id":"imposed-extreme","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"substantial-eligibility"},"source":["Dropout_Family = []\n","Dropout_Family.extend(Dropout_big_proba)\n","Dropout_Family.extend(Dropout_midium_proba)\n","Dropout_Family.extend(Dropout_small_proba)\n","\n","if len(Dropout_Family) == 15 :\n","    submission['ans'] = np.argmax(np.mean(np.concatenate(Dropout_Family, axis=2), axis=2), axis=1)\n","    submission.to_csv(f'custom_data/Dropout_Family/Dropout_Family.csv', index=False)\n","else :\n","    print('Something Wrong')"],"id":"substantial-eligibility","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"driving-wireless"},"source":["Dropout_Big = Dropout_big_proba[:]\n","\n","if len(Dropout_Big) == 5 :\n","    submission['ans'] = np.argmax(np.mean(np.concatenate(Dropout_Big, axis=2), axis=2), axis=1)\n","    submission.to_csv(f'custom_data/Dropout_Big/Dropout_Big.csv', index=False)\n","else :\n","    print('Something Wrong')"],"id":"driving-wireless","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"verbal-strain"},"source":["Dropout_Midium = Dropout_midium_proba[:]\n","\n","if len(Dropout_Midium) == 5 : \n","    submission['ans'] = np.argmax(np.mean(np.concatenate(Dropout_Midium, axis=2), axis=2), axis=1)\n","    submission.to_csv(f'custom_data/Dropout_Midium/Dropout_Midium.csv', index=False)\n","else :\n","    print('Something Wrong')"],"id":"verbal-strain","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bridal-parallel"},"source":["Dropout_Big_Midium = []\n","Dropout_Big_Midium.extend(Dropout_big_proba)\n","Dropout_Big_Midium.extend(Dropout_midium_proba)\n","\n","if len(Dropout_Big_Midium) == 10 :\n","    submission['ans'] = np.argmax(np.mean(np.concatenate(Dropout_Big_Midium, axis=2), axis=2), axis=1)\n","    submission.to_csv(f'custom_data/Dropout_Big_Midium/Dropout_Big_Midium.csv', index=False)\n","else :\n","    print('Something Wrong')"],"id":"bridal-parallel","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"prepared-decimal"},"source":["Dropout_m_s = []\n","Dropout_m_s.extend(Dropout_midium_proba)\n","Dropout_m_s.extend(Dropout_small_proba)\n","\n","if len(Dropout_m_s) == 10 :\n","    submission['ans'] = np.argmax(np.mean(np.concatenate(Dropout_m_s, axis=2), axis=2), axis=1)\n","    submission.to_csv(f'custom_data/Dropout_m_s/Dropout_m_s.csv', index=False)\n","else :\n","    print('Something Wrong')"],"id":"prepared-decimal","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"green-fundamental"},"source":["DM_DS_Nomal = []\n","DM_DS_Nomal.extend(Dropout_midium_proba)\n","DM_DS_Nomal.extend(Dropout_small_proba)\n","DM_DS_Nomal.extend(small_proba)\n","\n","if len(DM_DS_Nomal) == 15 :\n","    submission['ans'] = np.argmax(np.mean(np.concatenate(DM_DS_Nomal, axis=2), axis=2), axis=1)\n","    submission.to_csv(f'custom_data/DM_DS_Nomal/DM_DS_Nomal.csv', index=False)\n","else :\n","    print('Something Wrong')"],"id":"green-fundamental","execution_count":null,"outputs":[]}]}